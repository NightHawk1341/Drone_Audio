# Parser TextGrid VoiceStick - Audio-to-Command

Ce parser transforme les paires de fichiers TextGrid (transcriptions + commandes joystick) du corpus VoiceStick en un dataset pr√™t pour l'entra√Ænement de mod√®les Audio-to-Command.

## üéØ Objectif

Cr√©er un mapping direct **audio ‚Üí commande de drone** pour entra√Æner des mod√®les de classification (SVM, MLP) avec embeddings wav2vec2, sans passer par une transcription interm√©diaire.

## üìã Fonctionnalit√©s principales

### 1. Parsing automatique des TextGrid
- ‚úÖ Parse le TextGrid principal (transcriptions + distance √† cible)
- ‚úÖ Parse le TextGrid `_commands` (impulsions joystick)
- ‚úÖ Synchronisation temporelle automatique (tol√©rance configurable)

### 2. R√©solution d'ambigu√Øt√©s
- ‚úÖ **Gauche/Droite** : Distingue translation (`left`/`right`) vs rotation (`yawleft`/`yawright`)
- ‚úÖ **"Encore"** : G√®re les commandes de r√©p√©tition en se basant sur le joystick
- ‚úÖ **√ânonc√©s non-directifs** : Classe automatiquement en `none`

### 3. Classes de commandes
Le parser produit **9 classes** :
- `forward` : avancer
- `back` : reculer
- `left` : translation lat√©rale gauche
- `right` : translation lat√©rale droite
- `up` : monter
- `down` : descendre
- `yawleft` : rotation gauche
- `yawright` : rotation droite
- `none` : √©nonc√©s non-directifs (encouragements, h√©sitations, etc.)
- `stop` : arr√™t

## üöÄ Installation

```bash
pip install pandas --break-system-packages
```

Le parser ne n√©cessite aucune d√©pendance externe (parsing TextGrid manuel).

## üíª Utilisation

### Exemple basique

```python
from pathlib import Path
from voicestick_parser import VoiceStickParser

# Cr√©er le parser
parser = VoiceStickParser(tolerance=1.5)  # Fen√™tre temporelle en secondes

# Traiter une paire de fichiers
segments = parser.process_file_pair(
    main_tg_path=Path("fichier.TextGrid"),
    commands_tg_path=Path("fichier_commands.TextGrid"),
    audio_file="fichier.wav"
)

# Chaque segment contient:
# - audio_file: nom du fichier audio
# - start, end: timestamps en secondes
# - transcription: texte transcrit
# - command: classe de commande (forward, back, etc.)
# - joystick_match_confidence: 0-1, confiance du matching temporel
# - distance_to_target: distance √† la cible (optionnel)
```

### Traiter un corpus complet

```python
# Cr√©er un dataset CSV complet
df = parser.create_dataset(
    textgrid_dir=Path("/path/to/textgrids"),
    output_csv=Path("/path/to/output/dataset.csv"),
    audio_extension='.wav'
)

# Le CSV contient toutes les informations pour l'entra√Ænement
print(df.head())
```

### Structure du CSV de sortie

```
audio_file,start,end,duration,transcription,command,joystick_confidence,distance_to_target
01_04_25_11_19_02_000.wav,35.32,36.50,1.18,"vers ta droite",down,0.57,148.9
01_04_25_11_19_02_000.wav,36.89,37.86,0.97,"tourne l√©g√®rement √† droite",right,0.95,135.2
...
```

## üîç Logique de classification

### Priorit√©s de d√©cision

1. **Si joystick disponible** ‚Üí Priorit√© au joystick (source de v√©rit√©)
   - R√©sout l'ambigu√Øt√© gauche/droite automatiquement
   - G√®re les incoh√©rences transcription/action

2. **Si pas de joystick** ‚Üí Classification sur transcription
   - Mots-cl√©s spatiaux d√©tect√©s
   - Heuristiques pour rotation vs translation
   - √ânonc√©s non-directifs ‚Üí `none`

### Exemples de r√©solution d'ambigu√Øt√©

#### Exemple 1: Rotation d√©tect√©e
```
Transcription: "tourne l√©g√®rement √† droite"
Joystick: right (translation)
‚Üí R√©sultat: right
```

Avec le mot-cl√© "tourne", si pas de joystick on aurait inf√©r√© `yawright`, mais le joystick indique une translation.

#### Exemple 2: Incoh√©rence transcription/joystick
```
Transcription: "vers ta droite" 
Joystick: down
‚Üí R√©sultat: down (confiance au joystick)
```

Le pilote a mal interpr√©t√© ou le guide s'est tromp√© de direction.

#### Exemple 3: "Encore" avec r√©p√©tition
```
Transcription: "encore un peu √† droite"
Joystick: back
‚Üí R√©sultat: back
```

"Encore" r√©p√®te la derni√®re action, qui n'√©tait pas "droite" mais "back".

## ‚öôÔ∏è Configuration

### Param√®tre `tolerance`

```python
parser = VoiceStickParser(tolerance=1.5)
```

- **R√¥le** : Fen√™tre temporelle (secondes) pour chercher le joystick correspondant
- **D√©faut** : 1.5s
- **Impact** :
  - Trop petit ‚Üí Moins de matches, plus de classification sur transcription seule
  - Trop grand ‚Üí Risque de matches incorrects

### Ajuster les mots-cl√©s

Vous pouvez modifier `SPATIAL_KEYWORDS` et `NON_DIRECTIVE_KEYWORDS` dans la classe :

```python
parser.SPATIAL_KEYWORDS['forward'].append('allez')
parser.NON_DIRECTIVE_KEYWORDS.append('voil√†')
```

## üìä Analyse des r√©sultats

Le parser fournit des m√©triques de qualit√© :

```python
# Confidence moyenne du matching joystick
mean_conf = df['joystick_confidence'].mean()

# Segments sans match joystick
no_match = df[df['joystick_confidence'] == 0]

# Distribution des classes
print(df['command'].value_counts())
```

## üêõ Cas limites et gestion des erreurs

### Warnings g√©n√©r√©s

- `Fichier _commands manquant` : Pas de commandes joystick pour ce fichier
- `Tier 'Text' non trouv√©` : Structure TextGrid incorrecte
- `Erreur lors du parsing` : Fichier TextGrid corrompu

### Segments sans joystick

Environ 15-20% des segments peuvent ne pas avoir de match joystick :
- D√©calage temporel trop important
- Pas d'impulsion joystick (pause, h√©sitation)
- Segment en d√©but/fin d'enregistrement

Ces segments sont class√©s uniquement sur la transcription.

## üìà Statistiques typiques

Sur le corpus VoiceStick complet (~1470 commandes) :
- **Classe dominante** : `none` (~48%, √©nonc√©s non-directifs)
- **Classe la plus rare** : `yawleft`/`yawright` (<5%, rotations peu utilis√©es)
- **Confidence moyenne** : 0.6-0.7 (bon alignement temporel)

### D√©s√©quilibre des classes

Le d√©s√©quilibre important (stop ~18%, directions ~5-10% chacune) n√©cessitera :
- ‚úÖ Pond√©ration des classes lors de l'entra√Ænement
- ‚úÖ Data augmentation (pitch, vitesse, bruit)
- ‚úÖ Validation stratifi√©e

## üîß Int√©gration dans le pipeline

```
TextGrid parsing ‚Üí Segmentation audio ‚Üí Extraction embeddings ‚Üí Classification
     ‚Üì
   dataset.csv ‚Üê Vous √™tes ici
     ‚Üì
   D√©couper les WAV selon start/end
     ‚Üì
   wav2vec2 embeddings (frozen)
     ‚Üì
   SVM / MLP training
```

## üìù Format des donn√©es en sortie

Chaque ligne du CSV correspond √† **un segment audio labellis√©** pr√™t pour :

1. **Segmentation** : `audio_file`, `start`, `end` ‚Üí D√©couper le WAV
2. **Feature extraction** : Segment audio ‚Üí Embeddings wav2vec2
3. **Training** : Embeddings + `command` ‚Üí Entra√Æner classifieur

## üéì R√©f√©rences

- **Corpus VoiceStick** : Henry et al. (2025), PETRA'25
- **wav2vec2-FR-7K-large** : LeBenchmark, mod√®le fran√ßais pr√©-entra√Æn√©
- **Cahier des charges** : Projet M2 TAL, Univ. Grenoble Alpes

## ‚ö†Ô∏è Limitations connues

1. **Ambigu√Øt√© "gauche/droite" sans joystick** : Par d√©faut assume translation
2. **Commandes contextuelles** : "encore" sans contexte ‚Üí `none`
3. **Segmentation Whisper** : ~5% d'erreurs h√©rit√©es de la segmentation automatique
4. **G√©n√©ralisation** : Seulement 20 locuteurs dans le corpus

## üìû Support

Pour toute question sur l'utilisation du parser :
- V√©rifier les exemples dans `test_parser.py` et `demo_parser_detailed.py`
- Consulter le cahier des charges du projet
